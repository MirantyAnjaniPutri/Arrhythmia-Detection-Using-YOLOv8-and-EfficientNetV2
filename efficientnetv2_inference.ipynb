{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference EfficientNetV2 Tipe B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.applications import EfficientNetV2B2\n",
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from sklearn.metrics import average_precision_score, f1_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from helper_functions import create_tensorboard_callback\n",
    "# import time\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_and_preprocess_data(csv_path, image_folder, img_size, batch_size):\n",
    "#     # Read CSV\n",
    "#     try:\n",
    "#         df_merged = pd.read_csv(csv_path)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading CSV file: {e}\")\n",
    "#         return None, None, None\n",
    "\n",
    "#     if df_merged.empty or 'filename' not in df_merged.columns:\n",
    "#         print(\"CSV file is empty or does not contain the 'filename' column\")\n",
    "#         return None, None, None\n",
    "\n",
    "#     # Shuffle the DataFrame and limit it to the first 2000 rows\n",
    "#     df_merged = df_merged.sample(frac=1, random_state=42).head(2000)\n",
    "#     df_merged['filename'] = df_merged['filename'].apply(lambda x: os.path.join(image_folder, x + \".png\"))\n",
    "\n",
    "#     # Split dataset\n",
    "#     train_df, test_df = train_test_split(df_merged, test_size=0.2, random_state=42)\n",
    "#     test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "#     # Data generators\n",
    "#     train_datagen = ImageDataGenerator(\n",
    "#         rescale=1.0/255,\n",
    "#         rotation_range=20,\n",
    "#         width_shift_range=0.2,\n",
    "#         height_shift_range=0.2,\n",
    "#         shear_range=0.2,\n",
    "#         zoom_range=0.2,\n",
    "#         horizontal_flip=True,\n",
    "#         fill_mode='nearest'\n",
    "#     )\n",
    "#     validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "#     try:\n",
    "#         train_generator = train_datagen.flow_from_dataframe(\n",
    "#             dataframe=train_df,\n",
    "#             x_col='filename',\n",
    "#             y_col=train_df.columns[1:],\n",
    "#             target_size=(img_size, img_size),\n",
    "#             batch_size=batch_size,\n",
    "#             class_mode='raw'\n",
    "#         )\n",
    "#         validation_generator = validation_datagen.flow_from_dataframe(\n",
    "#             dataframe=val_df,\n",
    "#             x_col='filename',\n",
    "#             y_col=val_df.columns[1:],\n",
    "#             target_size=(img_size, img_size),\n",
    "#             batch_size=batch_size,\n",
    "#             class_mode='raw'\n",
    "#         )\n",
    "#         test_generator = validation_datagen.flow_from_dataframe(\n",
    "#             dataframe=test_df,\n",
    "#             x_col='filename',\n",
    "#             y_col=test_df.columns[1:],\n",
    "#             target_size=(img_size, img_size),\n",
    "#             batch_size=batch_size,\n",
    "#             class_mode='raw',\n",
    "#             shuffle=False\n",
    "#         )\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error creating data generators: {e}\")\n",
    "#         return None, None, None\n",
    "\n",
    "#     return train_generator, validation_generator, test_generator\n",
    "\n",
    "# def create_model(img_size, num_classes, base_weights_path):\n",
    "#     base_model = EfficientNetV2B2(weights=base_weights_path, include_top=False, input_tensor=Input(shape=(img_size, img_size, 3)))\n",
    "#     base_model.trainable = False\n",
    "\n",
    "#     inputs = tf.keras.layers.Input(shape=(img_size, img_size, 3), name='Input_layer')\n",
    "#     x = base_model(inputs, training=False)\n",
    "#     x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "#     outputs = tf.keras.layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "#     model = tf.keras.Model(inputs, outputs)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_model(model, test_generator):\n",
    "#     start_time = time.time()\n",
    "#     test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "#     end_time = time.time()\n",
    "\n",
    "#     inference_time = (end_time - start_time) / len(test_generator)\n",
    "\n",
    "#     # Get true labels and predictions\n",
    "#     y_true = test_generator.labels\n",
    "#     y_pred = model.predict(test_generator)\n",
    "    \n",
    "#     # Calculate mAP 0.5 and mAP 0.5:0.95\n",
    "#     average_precisions = []\n",
    "#     for i in range(y_true.shape[1]):\n",
    "#         average_precisions.append(average_precision_score(y_true[:, i], y_pred[:, i]))\n",
    "#     mAP_0_5 = np.mean(average_precisions)\n",
    "    \n",
    "#     # Convert predictions to binary\n",
    "#     y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "#     # Calculate F1-Score\n",
    "#     f1 = f1_score(y_true, y_pred_binary, average='macro')\n",
    "\n",
    "#     print(f\"Test Loss: {test_loss}\")\n",
    "#     print(f\"Test Accuracy: {test_accuracy}\")\n",
    "#     print(f\"mAP 0.5: {mAP_0_5}\")\n",
    "#     print(f\"F1-Score: {f1}\")\n",
    "#     print(f\"Inference Time (per image): {inference_time} seconds\")\n",
    "\n",
    "#     return mAP_0_5, f1, inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 validated image filenames.\n",
      "20/20 [==============================] - 51s 2s/step - loss: 0.6916 - binary_accuracy: 0.5333\n",
      "20/20 [==============================] - 43s 2s/step\n",
      "Test Loss: 0.6916007995605469\n",
      "Test Accuracy: 0.5333333611488342\n",
      "mAP 0.5: 0.49870922157158754\n",
      "mAP 0.5:0.95: 0.4998551042559242\n",
      "F1-Score: 0.38171091390450124\n",
      "Inference Time (per image): 2.626672101020813 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miran\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import average_precision_score, f1_score\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def load_and_preprocess_data(csv_path, image_folder, img_size, batch_size):\n",
    "    # Read CSV\n",
    "    try:\n",
    "        df_merged = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return None\n",
    "\n",
    "    if df_merged.empty or 'filename' not in df_merged.columns:\n",
    "        print(\"CSV file is empty or does not contain the 'filename' column\")\n",
    "        return None\n",
    "\n",
    "    # Shuffle the DataFrame and limit it to the first 2000 rows\n",
    "    df_merged = df_merged.sample(frac=1, random_state=42).head(2000)\n",
    "    df_merged['filename'] = df_merged['filename'].apply(lambda x: os.path.join(image_folder, x + \".png\"))\n",
    "\n",
    "    # Split dataset\n",
    "    _, test_df = train_test_split(df_merged, test_size=0.2, random_state=42)\n",
    "    test_df, _ = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Data generator\n",
    "    validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "    try:\n",
    "        test_generator = validation_datagen.flow_from_dataframe(\n",
    "            dataframe=test_df,\n",
    "            x_col='filename',\n",
    "            y_col=test_df.columns[1:],\n",
    "            target_size=(img_size, img_size),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='raw',\n",
    "            shuffle=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating data generators: {e}\")\n",
    "        return None\n",
    "\n",
    "    return test_generator\n",
    "\n",
    "def create_model(img_size, num_classes):\n",
    "    base_model = tf.keras.applications.EfficientNetV2B2(include_top=False, input_shape=(img_size, img_size, 3))\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(img_size, img_size, 3), name='Input_layer')\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    initial_learning_rate = 1e-4\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=100000,\n",
    "        decay_rate=1.0,\n",
    "        staircase=True)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "    return model\n",
    "\n",
    "def calculate_map(y_true, y_pred, thresholds):\n",
    "    average_precisions = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        aps = [average_precision_score(y_true[:, i], (y_pred[:, i] > t).astype(int)) for t in thresholds]\n",
    "        average_precisions.append(np.mean(aps))\n",
    "    return np.mean(average_precisions)\n",
    "\n",
    "def evaluate_model(model, checkpoint_path, test_generator):\n",
    "    # Load the saved weights\n",
    "    model.load_weights(checkpoint_path)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "    end_time = time.time()\n",
    "\n",
    "    inference_time = (end_time - start_time) / len(test_generator)\n",
    "\n",
    "    # Get true labels and predictions\n",
    "    y_true = test_generator.labels\n",
    "    y_pred = model.predict(test_generator)\n",
    "    \n",
    "    # Calculate mAP 0.5\n",
    "    mAP_0_5 = calculate_map(y_true, y_pred, [0.5])\n",
    "    \n",
    "    # Calculate mAP 0.5:0.95\n",
    "    mAP_0_5_0_95 = calculate_map(y_true, y_pred, np.arange(0.5, 1.0, 0.05))\n",
    "    \n",
    "    # Convert predictions to binary\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate F1-Score\n",
    "    f1 = f1_score(y_true, y_pred_binary, average='macro')\n",
    "\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    print(f\"mAP 0.5: {mAP_0_5}\")\n",
    "    print(f\"mAP 0.5:0.95: {mAP_0_5_0_95}\")\n",
    "    print(f\"F1-Score: {f1}\")\n",
    "    print(f\"Inference Time (per image): {inference_time} seconds\")\n",
    "\n",
    "    return mAP_0_5, mAP_0_5_0_95, f1, inference_time\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "csv_path = \"D:/Skripsweet/BISMILLAH_KELAR/dataset/image_datasets.csv\"  # Update with the actual path\n",
    "image_folder = \"D:/Skripsweet/BISMILLAH_KELAR/dataset/images\"\n",
    "img_size = 640\n",
    "batch_size = 10\n",
    "num_classes = 24\n",
    "\n",
    "test_generator = load_and_preprocess_data(csv_path, image_folder, img_size, batch_size)\n",
    "\n",
    "if test_generator:\n",
    "    model = create_model(img_size, num_classes)\n",
    "    checkpoint_path = \"D:/Skripsweet/BISMILLAH_KELAR/dataset/runs/best_top_layers.ckpt\"  # Path to your best model checkpoint files (index and data-0000-of-0001)\n",
    "    mAP_0_5, mAP_0_5_0_95, f1, inference_time = evaluate_model(model, checkpoint_path, test_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 validated image filenames.\n",
      "20/20 [==============================] - 28s 1s/step - loss: 0.6930 - binary_accuracy: 0.5248\n",
      "20/20 [==============================] - 11s 447ms/step\n",
      "Test Loss: 0.6930428147315979\n",
      "Test Accuracy: 0.5247916579246521\n",
      "mAP 0.5: 0.5018398288583915\n",
      "mAP 0.5:0.95: 0.5001839828858392\n",
      "F1-Score: 0.43899973296792627\n",
      "Inference Time (per image): 1.4475922107696533 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\miran\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "csv_path_binary = \"D:/Skripsweet/BISMILLAH_KELAR/dataset/datasets_binarize.csv\"  # Update with the actual path\n",
    "image_folder_binary = \"D:/Skripsweet/BISMILLAH_KELAR/dataset/binary_images\"\n",
    "img_size = 640\n",
    "batch_size = 10\n",
    "num_classes = 24\n",
    "\n",
    "test_generator = load_and_preprocess_data(csv_path_binary, image_folder_binary, img_size, batch_size)\n",
    "\n",
    "if test_generator:\n",
    "    model = create_model(img_size, num_classes)\n",
    "    checkpoint_path = \"D:/Skripsweet/BISMILLAH_KELAR/dataset/runs/best_top_layers.ckpt\"  # Path to your best model checkpoint files (index and data-0000-of-0001)\n",
    "    mAP_0_5, mAP_0_5_0_95, f1, inference_time = evaluate_model(model, checkpoint_path, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 validated image filenames.\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 40s 2s/step - loss: 0.6933 - binary_accuracy: 0.4633\n",
      "20/20 [==============================] - 23s 1s/step\n",
      "Test Loss: 0.6933253407478333\n",
      "Test Accuracy: 0.4633333384990692\n",
      "mAP 0.5: 0.5000835905349795\n",
      "mAP 0.5:0.95: 0.5000083590534979\n",
      "F1-Score: 0.34795729659869895\n",
      "Inference Time (per image): 2.090709412097931 seconds\n"
     ]
    }
   ],
   "source": [
    "csv_path = \"D:/Skripsweet/BISMILLAH_KELAR/dataset/image_datasets.csv\"  # Update with the actual path\n",
    "image_folder = \"D:/Skripsweet/BISMILLAH_KELAR/dataset/images\"\n",
    "img_size = 640\n",
    "batch_size = 10\n",
    "num_classes = 24\n",
    "\n",
    "test_generator = load_and_preprocess_data(csv_path, image_folder, img_size, batch_size)\n",
    "\n",
    "if test_generator:\n",
    "    model = create_model(img_size, num_classes)\n",
    "    # checkpoint_path = \"D:/Skripsweet/BISMILLAH_KELAR/dataset/runs/best_top_layers.ckpt\"  # Path to your best model checkpoint files (index and data-0000-of-0001)\n",
    "    checkpoint_path = \"D:/Skripsweet/BISMILLAH_KELAR/dataset/runs/best_all_layers.ckpt\"\n",
    "    mAP_0_5, mAP_0_5_0_95, f1, inference_time = evaluate_model(model, checkpoint_path, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 validated image filenames.\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 21s 931ms/step - loss: 0.6933 - binary_accuracy: 0.4658\n",
      "20/20 [==============================] - 11s 435ms/step\n",
      "Test Loss: 0.6933237314224243\n",
      "Test Accuracy: 0.4658333361148834\n",
      "mAP 0.5: 0.5008132573159579\n",
      "mAP 0.5:0.95: 0.5000813257315958\n",
      "F1-Score: 0.36434213174424696\n",
      "Inference Time (per image): 1.0774729013442994 seconds\n"
     ]
    }
   ],
   "source": [
    "csv_path_binary = \"D:/Skripsweet/BISMILLAH_KELAR/dataset/datasets_binarize.csv\"  # Update with the actual path\n",
    "image_folder_binary = \"D:/Skripsweet/BISMILLAH_KELAR/dataset/binary_images\"\n",
    "img_size = 640\n",
    "batch_size = 10\n",
    "num_classes = 24\n",
    "\n",
    "test_generator = load_and_preprocess_data(csv_path_binary, image_folder_binary, img_size, batch_size)\n",
    "\n",
    "if test_generator:\n",
    "    model = create_model(img_size, num_classes)\n",
    "    # checkpoint_path = \"D:/Skripsweet/BISMILLAH_KELAR/dataset/runs/best_top_layers.ckpt\"  # Path to your best model checkpoint files (index and data-0000-of-0001)\n",
    "    checkpoint_path = \"D:/Skripsweet/BISMILLAH_KELAR/dataset/runs/best_all_layers.ckpt\"\n",
    "    mAP_0_5, mAP_0_5_0_95, f1, inference_time = evaluate_model(model, checkpoint_path, test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(csv_path, image_folder, img_size, batch_size):\n",
    "    # Read CSV\n",
    "    try:\n",
    "        df_merged = pd.read_csv(csv_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    if df_merged.empty or 'filename' not in df_merged.columns:\n",
    "        print(\"CSV file is empty or does not contain the 'filename' column\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Shuffle the DataFrame and limit it to the first 2000 rows\n",
    "    df_merged = df_merged.sample(frac=1, random_state=42).head(2000)\n",
    "    df_merged['filename'] = df_merged['filename'].apply(lambda x: os.path.join(image_folder, x + \".png\"))\n",
    "\n",
    "    # Split dataset\n",
    "    train_df, test_df = train_test_split(df_merged, test_size=0.2, random_state=42)\n",
    "    test_df, val_df = train_test_split(test_df, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Data generators\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1.0/255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    validation_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "    try:\n",
    "        train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe=train_df,\n",
    "            x_col='filename',\n",
    "            y_col=train_df.columns[1:],\n",
    "            target_size=(img_size, img_size),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='raw'\n",
    "        )\n",
    "        validation_generator = validation_datagen.flow_from_dataframe(\n",
    "            dataframe=val_df,\n",
    "            x_col='filename',\n",
    "            y_col=val_df.columns[1:],\n",
    "            target_size=(img_size, img_size),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='raw'\n",
    "        )\n",
    "        test_generator = validation_datagen.flow_from_dataframe(\n",
    "            dataframe=test_df,\n",
    "            x_col='filename',\n",
    "            y_col=test_df.columns[1:],\n",
    "            target_size=(img_size, img_size),\n",
    "            batch_size=batch_size,\n",
    "            class_mode='raw',\n",
    "            shuffle=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating data generators: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "    return train_generator, validation_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_1(img_size, num_classes, base_weights_path):\n",
    "    base_model = tf.keras.applications.EfficientNetV2B2(include_top=False, input_shape=(img_size, img_size, 3))\n",
    "    base_model.trainable = False\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(img_size, img_size, 3), name='Input_layer')\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    initial_learning_rate = 1e-4\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=100000,\n",
    "        decay_rate=1.0,\n",
    "        staircase=True)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_all_layers(model, initial_learning_rate):\n",
    "    model.layers[1].trainable = True\n",
    "    for layer in model.layers[1].layers[:-60]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=100000,\n",
    "        decay_rate=1.0,\n",
    "        staircase=True)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_top_layers(model, initial_learning_rate):\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate,\n",
    "        decay_steps=100000,\n",
    "        decay_rate=1.0,\n",
    "        staircase=True)\n",
    "\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1600 validated image filenames.\n",
      "Found 200 validated image filenames.\n",
      "Found 200 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train_generator, validation_generator, test_generator = load_and_preprocess_data(csv_path, image_folder, img_size, batch_size)\n",
    "    \n",
    "num_classes = 24\n",
    "base_weights_path = r\"D:/Skripsweet/BISMILLAH_KELAR/efficientnetv2-b2_notop.h5\"\n",
    "model = create_model_1(img_size, num_classes, base_weights_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 640, 640, 3)]     0         \n",
      "                                                                 \n",
      " efficientnetv2-b2 (Function  (None, 20, 20, 1408)     8769374   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d_7   (None, 1408)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 24)                33816     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,803,190\n",
      "Trainable params: 2,735,016\n",
      "Non-trainable params: 6,068,174\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "no_weights = train_top_layers(model, 1e-4)\n",
    "no_weights.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input_layer (InputLayer)    [(None, 640, 640, 3)]     0         \n",
      "                                                                 \n",
      " efficientnetv2-b2 (Function  (None, 20, 20, 1408)     8769374   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d_7   (None, 1408)             0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 24)                33816     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 8,803,190\n",
      "Trainable params: 2,735,016\n",
      "Non-trainable params: 6,068,174\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "with_weights = train_all_layers(model, 1e-4)\n",
    "with_weights.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference EFFNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path_color = \"D:/Skripsweet/BISMILLAH_KELAR/datasets/images/val/13143_lr-0.png\"\n",
    "image_path_binarize = \"D:/Skripsweet/BISMILLAH_KELAR/datasets_binarize/val/images/13143_lr-0_1.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import load_img, img_to_array\n",
    "\n",
    "img = load_img(image_path_color, target_size=(640, 640)) # Replace img_width, img_height with your model's expected input size\n",
    "img_array_color = img_to_array(img)\n",
    "img_array_color = np.expand_dims(img_array_color, axis=0)  # Add batch dimension\n",
    "img_array_color = img_array_color / 255.0  # Normalize pixel values (if needed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_binary = load_img(image_path_binarize, target_size=(640, 640)) # Replace img_width, img_height with your model's expected input size\n",
    "img_array_binary = img_to_array(img_binary)\n",
    "img_array_binary = np.expand_dims(img_array_binary, axis=0)  # Add batch dimension\n",
    "img_array_binary = img_array_binary / 255.0  # Normalize pixel values (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class labels for display\n",
    "class_names = ['I arrhythmia', 'II arrhythmia', 'III arrhythmia', 'aVF arrhythmia', 'aVL arrhythmia', 'aVR arrhythmia',\n",
    "                'V1 arrhythmia', 'V2 arrhythmia', 'V3 arrhythmia', 'V4 arrhythmia', 'V5 arrhythmia', 'V6 arrhythmia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "[[0.4759252  0.47725397 0.47613168 0.47663113 0.476132   0.47583202\n",
      "  0.47584096 0.47579756 0.4757403  0.4754664  0.47685066 0.47542414\n",
      "  0.525833   0.5258463  0.52203983 0.52417153 0.52350175 0.52321243\n",
      "  0.5252535  0.52343076 0.52427536 0.52412486 0.5228706  0.524206  ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "# 1. Load the .h5 Model For EffNetV2 type B2 and No Pre-Trained Weights\n",
    "model_path_color_no_weights = 'D:/Skripsweet/BISMILLAH_KELAR/dataset/early_model_ori.h5'  # Replace with your actual path\n",
    "model_color_no_weights = keras.models.load_model(model_path_color_no_weights)\n",
    "\n",
    "# Make Predictions\n",
    "predictions_1 = model_color_no_weights.predict(img_array_color)\n",
    "\n",
    "print(predictions_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "[[0.4909087  0.4904405  0.49054766 0.4897278  0.49056837 0.49050903\n",
      "  0.49063584 0.49047273 0.49079096 0.4901032  0.4897929  0.490056\n",
      "  0.5095559  0.50909764 0.5101658  0.5100444  0.5096905  0.5094981\n",
      "  0.50969005 0.5094849  0.509218   0.50902426 0.50962055 0.50987536]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "# 1. Load the .h5 Model For EffNetV2 type B2 and No Pre-Trained Weights\n",
    "model_path_color_weights = 'D:/Skripsweet/BISMILLAH_KELAR/dataset/final_model_ori.h5'  # Replace with your actual path\n",
    "model_color_weights = keras.models.load_model(model_path_color_weights)\n",
    "\n",
    "# Make Predictions\n",
    "predictions_2 = model_color_weights.predict(img_array_color)\n",
    "\n",
    "print(predictions_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "[[0.46322677 0.462527   0.46235844 0.46241063 0.46240157 0.4625566\n",
      "  0.46225277 0.4625731  0.46229872 0.46240702 0.4619704  0.46293682\n",
      "  0.5368149  0.5377579  0.5382155  0.53768986 0.5374766  0.5368958\n",
      "  0.5378943  0.5382453  0.5373464  0.5366147  0.53751844 0.53716826]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "# 1. Load the .h5 Model For EffNetV2 type B2 and No Pre-Trained Weights\n",
    "model_path_binary_no_weights = 'D:/Skripsweet/BISMILLAH_KELAR/dataset/early_model_ori_binary.h5'  # Replace with your actual path\n",
    "model_binary_no_weights = keras.models.load_model(model_path_binary_no_weights)\n",
    "\n",
    "# Make Predictions\n",
    "predictions_3 = model_binary_no_weights.predict(img_array_binary)\n",
    "\n",
    "print(predictions_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 3s 3s/step\n",
      "[[0.48997414 0.49017546 0.48989487 0.49006242 0.4896534  0.49006423\n",
      "  0.48983675 0.490274   0.48963267 0.49004924 0.4900495  0.4897588\n",
      "  0.5101237  0.5098772  0.51021725 0.50992167 0.51035124 0.5104474\n",
      "  0.51025385 0.5103281  0.5103459  0.51019084 0.510339   0.5098905 ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "\n",
    "# 1. Load the .h5 Model For EffNetV2 type B2 and No Pre-Trained Weights\n",
    "model_path_binary_weights = 'D:/Skripsweet/BISMILLAH_KELAR/dataset/final_model_ori_binary.h5'  # Replace with your actual path\n",
    "model_binary_weights = keras.models.load_model(model_path_binary_weights)\n",
    "\n",
    "# Make Predictions\n",
    "predictions_4 = model_binary_weights.predict(img_array_binary)\n",
    "\n",
    "print(predictions_4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
